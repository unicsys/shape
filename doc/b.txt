# --- Find all PRIMARY KEY constraints and format them for readability ---

# Step 1: Run the original query to get the raw data
pk_query = """
SELECT
    table_name,
    constraint_text
FROM
    duckdb_constraints
WHERE
    constraint_type = 'PRIMARY KEY'
ORDER BY
    table_name;
"""

print("--- Finding all Primary Key constraints ---")
primary_key_df = analyzer.analyze(pk_query)

# Step 2: Process the raw DataFrame to create a clean visualization
if primary_key_df is not None and not primary_key_df.empty:
    
    # Create a copy of the DataFrame to modify, which is good practice
    clean_pk_df = primary_key_df.copy()
    
    # --- This is the key transformation ---
    # We apply a function to the 'constraint_text' column to clean it up.
    # It removes the 'PRIMARY KEY' text, strips whitespace, and removes parentheses.
    clean_pk_df['Key Columns'] = clean_pk_df['constraint_text'].str.replace('PRIMARY KEY', '', case=False).str.strip().str.strip('()')
    
    # We can now drop the original messy column
    clean_pk_df = clean_pk_df.drop(columns=['constraint_text'])
    
    # Rename 'table_name' for better readability
    clean_pk_df = clean_pk_df.rename(columns={'table_name': 'Table Name'})
    
    print("\n--- Official Table Unique Identifiers (Formatted View) ---")
    display(clean_pk_df[['Table Name', 'Key Columns']])

else:
    print("\nNo PRIMARY KEY constraints were found in this DDL.")


==============================

def find_tables_with_columns(analyzer, columns_to_find):
    """
    Finds all tables in the schema that contain ALL of the specified columns.

    Args:
        analyzer: An instance of the SQLAnalyzer class.
        columns_to_find: A list of column names to search for.

    Returns:
        A pandas DataFrame with the names of the tables found, or None if an error occurs.
    """
    if not columns_to_find:
        print("❌ Please provide a list of column names to search for.")
        return None

    # Format the list of columns for use in the SQL IN clause
    # e.g., ['COL_A', 'COL_B'] becomes "'COL_A', 'COL_B'"
    formatted_columns = ", ".join([f"'{col}'" for col in columns_to_find])
    
    # The number of columns we need to find in each table
    required_column_count = len(columns_to_find)

    # Dynamically build the query
    query = f"""
    SELECT
        table_name
    FROM
        information_schema.columns
    WHERE
        table_schema = 'HAMR'
        AND column_name IN ({formatted_columns})
    GROUP BY
        table_name
    HAVING
        -- This is the key: the count of distinct columns found must match our target count
        COUNT(DISTINCT column_name) = {required_column_count}
    ORDER BY
        table_name;
    """
    
    print(f"--- Searching for tables with ALL of the following columns: {columns_to_find} ---")
    result_df = analyzer.analyze(query)
    
    return result_df


===============

# --- Query 1: Find tables with all three specified columns ---

# Define the list of columns you want to find
cols_to_search_for = ['ENGINE_ITEM_PN', 'ENGINE_ITEM_SN', 'INSPECTION_TYPE']

# Use our new function to run the analysis
tables_found_df = find_tables_with_columns(analyzer, cols_to_search_for)

# Display the results
if tables_found_df is not None and not tables_found_df.empty:
    print(f"\n✅ Found {len(tables_found_df)} tables containing all specified columns:")
    display(tables_found_df)
else:
    print("\n❌ No tables were found containing all of the specified columns.")

=================

# --- Query 2: Find tables with a different set of two columns ---

# Just change the list of columns
cols_to_search_for_2 = ['ENG_ID', 'CUST_ID']

# Call the exact same function again
tables_found_df_2 = find_tables_with_columns(analyzer, cols_to_search_for_2)

# Display the results
if tables_found_df_2 is not None and not tables_found_df_2.empty:
    print(f"\n✅ Found {len(tables_found_df_2)} tables containing all specified columns:")
    display(tables_found_df_2)
else:
    print("\n❌ No tables were found containing all of the specified columns.")

